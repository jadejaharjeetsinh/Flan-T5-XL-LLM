{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17097cd8",
   "metadata": {},
   "source": [
    "# Query-Answer Validation using Flan-T5-XL\n",
    "### Author: Harjeetsinh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d169cf",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates how to use the Flan-T5-XL model from Hugging Face to determine whether an answer is correct for a given question. The output will be 'YES' or 'NO'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install torch transformers sentencepiece --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dfbd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_decoded_output(decoded_output):\n",
    "    cleaned = decoded_output.strip().upper()\n",
    "    if \"YES\" in cleaned:\n",
    "        return \"YES\"\n",
    "    elif \"NO\" in cleaned:\n",
    "        return \"NO\"\n",
    "    else:\n",
    "        return \"NO\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llm_function(model, tokenizer, a, b):\n",
    "    prompt = f\"Question: {a}\\nAnswer: {b}\\nIs the answer correct? Say YES or NO.\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=5)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return clean_decoded_output(decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "question = \"What is the capital of France?\"\n",
    "answer = \"Paris is the capital of France.\"\n",
    "result = llm_function(model, tokenizer, question, answer)\n",
    "print(f\"Q: {question}\\nA: {answer}\\nOutput: {result}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
